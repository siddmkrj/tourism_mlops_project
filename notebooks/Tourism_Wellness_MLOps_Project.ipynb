{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "klg2JF-oBblG"
      },
      "source": [
        "# Problem Statement & Objective"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m0CcOjZ-BblL"
      },
      "source": [
        "## **Business Context**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uyT6Koe7BblM"
      },
      "source": [
        "\"Visit with Us,\" a leading travel company, is revolutionizing the tourism industry by leveraging data-driven strategies to optimize operations and customer engagement. While introducing a new package offering, such as the Wellness Tourism Package, the company faces challenges in targeting the right customers efficiently. The manual approach to identifying potential customers is inconsistent, time-consuming, and prone to errors, leading to missed opportunities and suboptimal campaign performance.\n",
        "\n",
        "To address these issues, the company aims to implement a scalable and automated system that integrates customer data, predicts potential buyers, and enhances decision-making for marketing strategies. By utilizing an MLOps pipeline, the company seeks to achieve seamless integration of data preprocessing, model development, deployment, and CI/CD practices for continuous improvement. This system will ensure efficient targeting of customers, timely updates to the predictive model, and adaptation to evolving customer behaviors, ultimately driving growth and customer satisfaction.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zm6bNQOJBblO"
      },
      "source": [
        "## **Objective**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7PYtjk_YBblO"
      },
      "source": [
        "As an MLOps Engineer at \"Visit with Us,\" your responsibility is to design and deploy an MLOps pipeline on GitHub to automate the end-to-end workflow for predicting customer purchases. The primary objective is to build a model that predicts whether a customer will purchase the newly introduced Wellness Tourism Package before contacting them. The pipeline will include data cleaning, preprocessing, transformation, model building, training, evaluation, and deployment, ensuring consistent performance and scalability. By leveraging GitHub Actions for CI/CD integration, the system will enable automated updates, streamline model deployment, and improve operational efficiency. This robust predictive solution will empower policymakers to make data-driven decisions, enhance marketing strategies, and effectively target potential customers, thereby driving customer acquisition and business growth."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z8C11AzTBblP"
      },
      "source": [
        "## **Data Description**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9DQx3pkaBblP"
      },
      "source": [
        "The dataset contains customer and interaction data that serve as key attributes for predicting the likelihood of purchasing the Wellness Tourism Package. The detailed attributes are:\n",
        "\n",
        "**Customer Details**\n",
        "- **CustomerID:** Unique identifier for each customer.\n",
        "- **ProdTaken:** Target variable indicating whether the customer has purchased a package (0: No, 1: Yes).\n",
        "- **Age:** Age of the customer.\n",
        "- **TypeofContact:** The method by which the customer was contacted (Company Invited or Self Inquiry).\n",
        "- **CityTier:** The city category based on development, population, and living standards (Tier 1 > Tier 2 > Tier 3).\n",
        "- **Occupation:** Customer's occupation (e.g., Salaried, Freelancer).\n",
        "- **Gender:** Gender of the customer (Male, Female).\n",
        "- **NumberOfPersonVisiting:** Total number of people accompanying the customer on the trip.\n",
        "- **PreferredPropertyStar:** Preferred hotel rating by the customer.\n",
        "- **MaritalStatus:** Marital status of the customer (Single, Married, Divorced).\n",
        "- **NumberOfTrips:** Average number of trips the customer takes annually.\n",
        "- **Passport:** Whether the customer holds a valid passport (0: No, 1: Yes).\n",
        "- **OwnCar:** Whether the customer owns a car (0: No, 1: Yes).\n",
        "- **NumberOfChildrenVisiting:** Number of children below age 5 accompanying the customer.\n",
        "- **Designation:** Customer's designation in their current organization.\n",
        "- **MonthlyIncome:** Gross monthly income of the customer.\n",
        "\n",
        "**Customer Interaction Data**\n",
        "- **PitchSatisfactionScore:** Score indicating the customer's satisfaction with the sales pitch.\n",
        "- **ProductPitched:** The type of product pitched to the customer.\n",
        "- **NumberOfFollowups:** Total number of follow-ups by the salesperson after the sales pitch.-\n",
        "- **DurationOfPitch:** Duration of the sales pitch delivered to the customer.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Setup Instructions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1. Created Conda Environment\n",
        "\n",
        "I created and activated a conda environment:\n",
        "\n",
        "```bash\n",
        "conda create -n tourism-mlops python=3.10 -y\n",
        "conda activate tourism-mlops\n",
        "```\n",
        "\n",
        "### 2. Installed Requirements\n",
        "\n",
        "I installed the project dependencies:\n",
        "\n",
        "```bash\n",
        "pip install -r requirements.txt\n",
        "```\n",
        "\n",
        "### 3. Set Up Hugging Face\n",
        "\n",
        "#### Installed Hugging Face CLI\n",
        "\n",
        "I installed the Hugging Face CLI:\n",
        "\n",
        "```bash\n",
        "curl -LsSf https://hf.co/cli/install.sh | bash\n",
        "```\n",
        "\n",
        "#### Created Hugging Face Account & Token\n",
        "\n",
        "I completed the following steps:\n",
        "\n",
        "1. Went to [huggingface.co](https://huggingface.co) and signed in / signed up\n",
        "2. Clicked my profile → Settings → Access Tokens\n",
        "3. Created a New token (type: Write) and copied it\n",
        "\n",
        "#### Logged In from Terminal\n",
        "\n",
        "I logged in from the terminal (inside the conda environment):\n",
        "\n",
        "```bash\n",
        "huggingface-cli login\n",
        "```\n",
        "\n",
        "I pasted my token when prompted.\n",
        "\n",
        "#### Created Dataset Repository on Hugging Face\n",
        "\n",
        "I created the dataset repository in my browser:\n",
        "\n",
        "1. Went to [huggingface.co/datasets](https://huggingface.co/datasets)\n",
        "2. Clicked **New dataset**\n",
        "3. Named it: `mukherjee78/tourism-wellness-package`\n",
        "4. Set visibility to **Public**\n",
        "5. Clicked **Create**\n",
        "\n",
        "### 4. Created Project Structure\n",
        "\n",
        "I created the project folder structure:\n",
        "\n",
        "```bash\n",
        "mkdir data notebooks src\n",
        "```\n",
        "\n",
        "Then I:\n",
        "1. Created a notebook inside the `notebooks` folder\n",
        "2. Copied the `tourism.csv` file into the `data` folder"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9DtS3gNDjBbR"
      },
      "source": [
        "# Data Registration (Hugging Face Datasets)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "HF_USERNAME = \"mukherjee78\"\n",
        "DATASET_REPO_ID = f\"{HF_USERNAME}/tourism-wellness-package\"\n",
        "MODEL_REPO_ID = f\"{HF_USERNAME}/tourism-wellness-best-model\"\n",
        "HF_SPACE_REPO_ID = f\"{HF_USERNAME}/tourism-wellness-space\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kNUYcTe-xckI"
      },
      "outputs": [],
      "source": [
        "from huggingface_hub import HfApi\n",
        "import os\n",
        "\n",
        "api = HfApi()\n",
        "\n",
        "local_data_path = \"../data/tourism.csv\"\n",
        "\n",
        "api.upload_file(\n",
        "    path_or_fileobj=local_data_path,\n",
        "    path_in_repo=\"data/tourism.csv\",\n",
        "    repo_id=DATASET_REPO_ID,\n",
        "    repo_type=\"dataset\"\n",
        ")\n",
        "\n",
        "print(\"Uploaded tourism.csv to Hugging Face Datasets repo:\", DATASET_REPO_ID)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WxXiD9ZXxodF"
      },
      "source": [
        "> We created a Hugging Face Dataset repository mukherjee78/tourism-wellness-package and uploaded the raw tourism.csv file to it. This satisfies the data registration requirement and allows the rest of the pipeline to load data directly from the data hub."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SEG3M03Y9dn7"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "dataset = load_dataset(DATASET_REPO_ID, data_files={\"full\": \"data/tourism.csv\"})\n",
        "dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = dataset[\"full\"].to_pandas()\n",
        "df.head()\n",
        "df.info()\n",
        "df.describe(include=\"all\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hh2TjRG5WJ4Z"
      },
      "source": [
        "## Data Preparation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Load the dataset from Hugging Face"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "dataset = load_dataset(DATASET_REPO_ID, data_files={\"full\": \"data/tourism.csv\"})\n",
        "df = dataset[\"full\"].to_pandas()\n",
        "\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Basic data inspection (for explanation + cleaning decisions)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"Shape:\", df.shape)\n",
        "print(\"\\nColumns:\\n\", df.columns.tolist())\n",
        "\n",
        "print(\"\\nInfo:\")\n",
        "print(df.info())\n",
        "\n",
        "print(\"\\nMissing values per column:\")\n",
        "print(df.isna().sum())\n",
        "\n",
        "print(\"\\nNumber of duplicate rows:\", df.duplicated().sum())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Data cleaning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "TARGET_COL = \"ProdTaken\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.impute import SimpleImputer\n",
        "\n",
        "df_clean = df.copy()\n",
        "\n",
        "cols_to_drop = [\"CustomerID\", \"Unnamed: 0\"]\n",
        "\n",
        "df_clean = df_clean.drop(columns=cols_to_drop)\n",
        "print(f\"Dropped columns: {cols_to_drop}\")\n",
        "\n",
        "# Drop duplicates\n",
        "before = df_clean.shape[0]\n",
        "df_clean = df_clean.drop_duplicates()\n",
        "after = df_clean.shape[0]\n",
        "print(f\"Dropped {before - after} duplicate rows\")\n",
        "\n",
        "# Impute missing values\n",
        "feature_cols = [c for c in df_clean.columns if c != TARGET_COL]\n",
        "numeric_cols = df_clean[feature_cols].select_dtypes(include=[np.number]).columns.tolist()\n",
        "categorical_cols = df_clean[feature_cols].select_dtypes(exclude=[np.number]).columns.tolist()\n",
        "\n",
        "df_imputed = df_clean.copy()\n",
        "\n",
        "if numeric_cols:\n",
        "    num_imputer = SimpleImputer(strategy=\"median\")\n",
        "    df_imputed[numeric_cols] = num_imputer.fit_transform(df_imputed[numeric_cols])\n",
        "\n",
        "if categorical_cols:\n",
        "    cat_imputer = SimpleImputer(strategy=\"most_frequent\")\n",
        "    df_imputed[categorical_cols] = cat_imputer.fit_transform(df_imputed[categorical_cols])\n",
        "\n",
        "print(\"Remaining missing values after imputation:\", df_imputed.isna().sum().sum())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Train–test split and save locally"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X = df_imputed.drop(columns=[TARGET_COL])\n",
        "y = df_imputed[TARGET_COL]\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y,\n",
        "    test_size=0.2,\n",
        "    random_state=42,\n",
        "    stratify=y\n",
        ")\n",
        "\n",
        "print(\"Train shape:\", X_train.shape, y_train.shape)\n",
        "print(\"Test shape:\", X_test.shape, y_test.shape)\n",
        "\n",
        "train_df = X_train.copy()\n",
        "train_df[TARGET_COL] = y_train\n",
        "existing_cols_to_drop = [col for col in cols_to_drop if col in X_train.columns]\n",
        "if existing_cols_to_drop:\n",
        "    print(f\"Warning: Dropping identifier columns that shouldn't be present: {existing_cols_to_drop}\")\n",
        "    X_train = X_train.drop(columns=existing_cols_to_drop)\n",
        "\n",
        "\n",
        "test_df = X_test.copy()\n",
        "test_df[TARGET_COL] = y_test\n",
        "existing_cols_to_drop = [col for col in cols_to_drop if col in X_test.columns]\n",
        "if existing_cols_to_drop:\n",
        "    print(f\"Warning: Dropping identifier columns that shouldn't be present: {existing_cols_to_drop}\")\n",
        "    X_test = X_test.drop(columns=existing_cols_to_drop)\n",
        "\n",
        "train_path = \"../data/train.csv\" \n",
        "test_path  = \"../data/test.csv\"\n",
        "\n",
        "train_df.to_csv(train_path, index=False)\n",
        "test_df.to_csv(test_path, index=False)\n",
        "\n",
        "print(f\"Saved train to {train_path}, shape={train_df.shape}\")\n",
        "print(f\"Saved test to {test_path}, shape={test_df.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pd.read_csv(train_path).head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pd.read_csv(test_path).head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Upload train.csv and test.csv back to Hugging Face Dataset Space"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from huggingface_hub import HfApi\n",
        "\n",
        "api = HfApi()\n",
        "\n",
        "api.upload_file(\n",
        "    path_or_fileobj=train_path,\n",
        "    path_in_repo=\"data/train.csv\",\n",
        "    repo_id=DATASET_REPO_ID,\n",
        "    repo_type=\"dataset\",\n",
        ")\n",
        "\n",
        "api.upload_file(\n",
        "    path_or_fileobj=test_path,\n",
        "    path_in_repo=\"data/test.csv\",\n",
        "    repo_id=DATASET_REPO_ID,\n",
        "    repo_type=\"dataset\",\n",
        ")\n",
        "\n",
        "print(\"Uploaded train.csv and test.csv to HF dataset repo:\", DATASET_REPO_ID)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eZZKnLkLjeM4"
      },
      "source": [
        "# Modeling & Experiment Tracking"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Load Train/Test From Hugging Face Dataset Space"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "import pandas as pd\n",
        "\n",
        "dataset_splits = load_dataset(\n",
        "    DATASET_REPO_ID,\n",
        "    data_files={\n",
        "        \"train\": \"data/train.csv\",\n",
        "        \"test\": \"data/test.csv\"\n",
        "    }\n",
        ")\n",
        "\n",
        "train_df = dataset_splits[\"train\"].to_pandas()\n",
        "test_df = dataset_splits[\"test\"].to_pandas()\n",
        "\n",
        "print(\"Train shape:\", train_df.shape)\n",
        "print(\"Test shape:\", test_df.shape)\n",
        "\n",
        "train_df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Prepare Features and Target"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "X_train = train_df.drop(columns=[TARGET_COL])\n",
        "y_train = train_df[TARGET_COL]\n",
        "print(\"X_train shape:\", X_train.shape)\n",
        "print(\"y_train shape:\", y_train.shape)\n",
        "\n",
        "\n",
        "X_test = test_df.drop(columns=[TARGET_COL])\n",
        "y_test = test_df[TARGET_COL]\n",
        "print(\"X_test shape:\", X_test.shape)\n",
        "print(\"y_test shape:\", y_test.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Preprocessing Pipeline (Categorical Encoding + Numeric Passthrough)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "numeric_cols = X_train.select_dtypes(include=[np.number]).columns.tolist()\n",
        "categorical_cols = X_train.select_dtypes(exclude=[np.number]).columns.tolist()\n",
        "\n",
        "print(\"Numeric columns:\", numeric_cols)\n",
        "print(\"Categorical columns:\", categorical_cols)\n",
        "\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        (\"num\", \"passthrough\", numeric_cols),\n",
        "        (\"cat\", OneHotEncoder(handle_unknown='ignore'), categorical_cols),\n",
        "    ]\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### RandomForest Model + Tuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from scipy.stats import randint\n",
        "\n",
        "rf_model = RandomForestClassifier(random_state=42, n_jobs=-1)\n",
        "\n",
        "rf_pipe = Pipeline(steps=[\n",
        "    (\"preprocess\", preprocessor),\n",
        "    (\"model\", rf_model),\n",
        "])\n",
        "\n",
        "rf_param_dist = {\n",
        "    \"model__n_estimators\": randint(100, 400),\n",
        "    \"model__max_depth\": [None, 5, 10, 20],\n",
        "    \"model__min_samples_split\": randint(2, 10),\n",
        "    \"model__min_samples_leaf\": randint(1, 5),\n",
        "    \"model__max_features\": [\"sqrt\", \"log2\"],\n",
        "}\n",
        "\n",
        "rf_search = RandomizedSearchCV(\n",
        "    rf_pipe,\n",
        "    rf_param_dist,\n",
        "    n_iter=20,\n",
        "    scoring=\"f1\",\n",
        "    cv=3,\n",
        "    n_jobs=-1,\n",
        "    verbose=2,\n",
        "    random_state=42,\n",
        ")\n",
        "\n",
        "rf_search.fit(X_train, y_train)\n",
        "rf_best = rf_search.best_estimator_\n",
        "rf_best_params = rf_search.best_params_\n",
        "rf_search.best_score_, rf_best_params"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### XGBoost Model + Tuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from xgboost import XGBClassifier\n",
        "\n",
        "xgb_model = XGBClassifier(\n",
        "    objective=\"binary:logistic\",\n",
        "    eval_metric=\"logloss\",\n",
        "    random_state=42,\n",
        "    n_estimators=300\n",
        ")\n",
        "\n",
        "xgb_pipe = Pipeline(steps=[\n",
        "    (\"preprocess\", preprocessor),\n",
        "    (\"model\", xgb_model),\n",
        "])\n",
        "\n",
        "xgb_param_dist = {\n",
        "    \"model__learning_rate\": [0.01, 0.05, 0.1],\n",
        "    \"model__max_depth\": [3, 5, 7],\n",
        "    \"model__subsample\": [0.6, 0.8, 1.0],\n",
        "    \"model__colsample_bytree\": [0.6, 0.8, 1.0],\n",
        "}\n",
        "\n",
        "xgb_search = RandomizedSearchCV(\n",
        "    xgb_pipe,\n",
        "    xgb_param_dist,\n",
        "    n_iter=10,\n",
        "    scoring=\"f1\",\n",
        "    cv=3,\n",
        "    n_jobs=-1,\n",
        "    verbose=2,\n",
        "    random_state=42,\n",
        ")\n",
        "\n",
        "xgb_search.fit(X_train, y_train)\n",
        "xgb_best = xgb_search.best_estimator_\n",
        "xgb_best_params = xgb_search.best_params_\n",
        "xgb_search.best_score_, xgb_best_params"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Evaluate Both Models on the Test Set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
        "\n",
        "def evaluate(model, X_test, y_test):\n",
        "    y_pred = model.predict(X_test)\n",
        "    y_prob = model.predict_proba(X_test)[:, 1]\n",
        "    \n",
        "    return {\n",
        "        \"accuracy\": accuracy_score(y_test, y_pred),\n",
        "        \"precision\": precision_score(y_test, y_pred, zero_division=0),\n",
        "        \"recall\": recall_score(y_test, y_pred, zero_division=0),\n",
        "        \"f1\": f1_score(y_test, y_pred, zero_division=0),\n",
        "        \"roc_auc\": roc_auc_score(y_test, y_prob)\n",
        "    }\n",
        "\n",
        "rf_metrics = evaluate(rf_best, X_test, y_test)\n",
        "xgb_metrics = evaluate(xgb_best, X_test, y_test)\n",
        "\n",
        "rf_metrics, xgb_metrics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Compare Both Models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "comparison_df = pd.DataFrame([rf_metrics, xgb_metrics], index=[\"RandomForest\", \"XGBoost\"])\n",
        "comparison_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "> XGBoost achieved higher recall and F1-score, making it more suitable for identifying potential buyers. Therefore, XGBoost is selected as the final model for deployment."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Experiment Tracking Using MLflow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import mlflow\n",
        "import mlflow.sklearn\n",
        "\n",
        "mlflow.set_experiment(\"tourism_wellness_modeling\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Log RandomForest"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "with mlflow.start_run(run_name=\"RandomForest_Best\"):\n",
        "    mlflow.log_params(rf_best_params)\n",
        "    for k, v in rf_metrics.items():\n",
        "        mlflow.log_metric(k, v)\n",
        "    mlflow.sklearn.log_model(rf_best, name=\"rf_model\")\n",
        "    print(\"✅ Logged to MLflow with run name: RandomForest_Best\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Log XGBoost"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "with mlflow.start_run(run_name=\"XGBoost_Best\"):\n",
        "    mlflow.log_params(xgb_best_params)\n",
        "    for k, v in xgb_metrics.items():\n",
        "        mlflow.log_metric(k, v)\n",
        "    mlflow.sklearn.log_model(xgb_best, name=\"xgb_model\")\n",
        "    print(\"✅ Logged to MLflow with run name: XGBoost_Best\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Select Best Model & Save Locally"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "best_model_name = \"XGBoost\" if xgb_metrics[\"f1\"] > rf_metrics[\"f1\"] else \"RandomForest\"\n",
        "best_model = xgb_best if best_model_name == \"XGBoost\" else rf_best\n",
        "\n",
        "print(\"Best model selected:\", best_model_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import joblib, os\n",
        "\n",
        "os.makedirs(\"../models\", exist_ok=True)\n",
        "model_path = f\"../models/best_model.pkl\"\n",
        "joblib.dump(best_model, model_path)\n",
        "\n",
        "model_path"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Register Best Model on Hugging Face Model Hub"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from huggingface_hub import HfApi, create_repo\n",
        "\n",
        "HF_MODEL_REPO_ID = f\"{HF_USERNAME}/tourism-wellness-best-model\"\n",
        "\n",
        "create_repo(repo_id=HF_MODEL_REPO_ID, repo_type=\"model\", exist_ok=True)\n",
        "\n",
        "api = HfApi()\n",
        "\n",
        "api.upload_file(\n",
        "    path_or_fileobj=model_path,\n",
        "    path_in_repo=\"best_model.pkl\",\n",
        "    repo_id=HF_MODEL_REPO_ID,\n",
        "    repo_type=\"model\"\n",
        ")\n",
        "\n",
        "print(\"Model uploaded to:\", HF_MODEL_REPO_ID)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Model Deployment (HF Spaces)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Dockerfile"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Already created in src/Dockerfile\n",
        "with open(\"../src/Dockerfile\", \"r\") as f:\n",
        "    dockerfile_content = f.read()\n",
        "    print(dockerfile_content)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Streamlit App"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Already created in src/app.py\n",
        "with open(\"../src/app.py\", \"r\") as f:\n",
        "    app_content = f.read()\n",
        "    print(app_content)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Dependency Handling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Already created in src/requirements.txt\n",
        "with open(\"../src/requirements.txt\", \"r\") as f:\n",
        "    requirements_content = f.read()\n",
        "    print(requirements_content)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Hosting in HF"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "api = HfApi()\n",
        "\n",
        "api.create_repo(\n",
        "    repo_id=HF_SPACE_REPO_ID,\n",
        "    repo_type=\"space\",\n",
        "    exist_ok=True,\n",
        "    space_sdk=\"docker\"\n",
        ")\n",
        "\n",
        "files_to_upload = [\n",
        "    (f\"../src/Dockerfile\", \"Dockerfile\"),\n",
        "    (f\"../src/app.py\", \"app.py\"),\n",
        "    (f\"../src/requirements.txt\", \"requirements.txt\"),\n",
        "]\n",
        "\n",
        "for local_path, remote_path in files_to_upload:\n",
        "    print(f\"Uploading {local_path} to {HF_SPACE_REPO_ID}:{remote_path}\")\n",
        "    api.upload_file(\n",
        "        path_or_fileobj=local_path,\n",
        "        path_in_repo=remote_path,\n",
        "        repo_id=HF_SPACE_REPO_ID,\n",
        "        repo_type=\"space\"\n",
        "    )\n",
        "\n",
        "print(f\"✅ Deployment files pushed to Hugging Face Space: {HF_SPACE_REPO_ID}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## CI/CD with GitHub Actions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Already created in .github/workflows/pipeline.yml\n",
        "with open(\"../.github/workflows/pipeline.yml\", \"r\") as f:\n",
        "    pipeline_content = f.read()\n",
        "    print(pipeline_content)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Final Results & Links"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### GitHub (link to repository, screenshot of folder structure and executed workflow)\n",
        "\n",
        "[https://github.com/siddmkrj/tourism_mlops_project](https://github.com/siddmkrj/tourism_mlops_project)\n",
        "\n",
        "\n",
        "\n",
        "### Screenshot: Folder Structure\n",
        "\n",
        "![Folder](../git_folders.png)\n",
        "\n",
        "\n",
        "### Screenshot: Workflow\n",
        "\n",
        "![Workflow](../workflow.png)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Streamlit on Hugging Face (link to HF space, screenshot of Streamlit app)\n",
        "\n",
        "[https://huggingface.co/spaces/mukherjee78/tourism-wellness-space](https://huggingface.co/spaces/mukherjee78/tourism-wellness-space)\n",
        "\n",
        "\n",
        "\n",
        "### Screenshot: Streamlit app\n",
        "\n",
        "![Streamlit app](../streamlit.png)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "klg2JF-oBblG",
        "m0CcOjZ-BblL",
        "zm6bNQOJBblO",
        "z8C11AzTBblP",
        "0LbSu_p2jYfe",
        "9DtS3gNDjBbR",
        "hh2TjRG5WJ4Z",
        "eZZKnLkLjeM4",
        "0McYCZzkji5I",
        "9QrY2v77vbEZ",
        "LCvrklrBwNvJ",
        "07cYzWcIwTL-",
        "V4ynzpKNwWS_",
        "PuCgAW2hktli",
        "PvEUJ-t5kdxH",
        "BA6mP-Ebkm3O",
        "v-i8Jdyz-_L1"
      ],
      "provenance": []
    },
    "kernel_info": {
      "name": "python310-sdkv2"
    },
    "kernelspec": {
      "display_name": "tourism-mlops",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.19"
    },
    "microsoft": {
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      },
      "ms_spell_check": {
        "ms_spell_check_language": "en"
      }
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
